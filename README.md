# MCP Adapter Generator

Turn any API into an AI-callable MCP server â€” automatically.

Point it at a **Swagger URL**, **OpenAPI spec**, or **Postman collection**, and it generates a production-ready [Dedalus MCP](https://docs.dedaluslabs.ai/dmcp) server with proper schemas, safety policies, and deployment files.

Optionally uses **AI reasoning** (K2 / Dedalus) to enhance tool names, descriptions, and parameter metadata.

---

## How It Works â€” The Full Workflow

```
  Swagger URL or file          AI Reasoning             Code Generation
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  1. INGEST   â”‚â”€â”€â–¶â”‚ 2. MINE      â”‚â”€â”€â–¶â”‚ 3. REASON  â”‚â”€â”€â–¶â”‚ 4. SAFETY    â”‚â”€â”€â–¶â”‚ 5. CODEGEN â”‚
  â”‚              â”‚   â”‚              â”‚   â”‚  (K2 / AI)  â”‚   â”‚              â”‚   â”‚            â”‚
  â”‚ Fetch spec   â”‚   â”‚ Cluster into â”‚   â”‚ Enhance     â”‚   â”‚ Classify     â”‚   â”‚ server.py  â”‚
  â”‚ from URL or  â”‚   â”‚ high-level   â”‚   â”‚ names,      â”‚   â”‚ read/write/  â”‚   â”‚ tests      â”‚
  â”‚ local file   â”‚   â”‚ tools        â”‚   â”‚ descriptionsâ”‚   â”‚ destructive  â”‚   â”‚ .env       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                                                         â”‚
          â–¼                                                                         â–¼
  http://host/openapi.json                                              output/my-server/
  examples/petstore.yaml                                                 â”œâ”€â”€ server.py
  collection.json                                                        â”œâ”€â”€ main.py
                                                                         â”œâ”€â”€ test_server.py
                                                                         â”œâ”€â”€ pyproject.toml
                                                                         â”œâ”€â”€ requirements.txt
                                                                         â””â”€â”€ .env.example
```

### Pipeline Stages (with logging)

Every stage emits structured, timestamped logs so you can see exactly what happens:

1. **Ingestion** â€” Fetches the spec from a Swagger URL or reads a local file. Parses OpenAPI 3.x / Swagger 2.x / Postman v2.1 into a canonical `APISpec` model.
2. **Capability Mining** â€” Groups endpoints by tag/resource, clusters similar GET endpoints into unified search tools, generates snake_case names.
3. **AI Reasoning** *(optional, `--use-k2`)* â€” Sends tools to K2 (MBZUAI IFM) or Dedalus API for enhancement. The AI improves names, descriptions, param docs, and safety classification. Falls back gracefully if a provider is unreachable.
4. **Safety Classification** â€” Classifies each tool as read/write/destructive using HTTP method + keyword analysis. Applies allowlist/denylist, blocks destructive tools, redacts PII fields, adds safety badges.
5. **Code Generation** â€” Produces a complete Python MCP server (using `dedalus_mcp`), plus tests, deployment files (`main.py`, `pyproject.toml`), and environment config.

---

## Quick Start

```bash
pip install -r requirements.txt
```

### From a Swagger URL (recommended)

```bash
# Start your API (example: our test math app)
python test_application/app.py  # serves Swagger at http://127.0.0.1:8001/docs

# Generate MCP server from the live Swagger endpoint
python -m mcp_adapter generate \
  --url http://127.0.0.1:8001/openapi.json \
  --output output/math-api-mcp \
  --name math-api

# With AI-enhanced schemas
python -m mcp_adapter generate \
  --url http://127.0.0.1:8001/openapi.json \
  --output output/math-api-mcp \
  --name math-api \
  --use-k2
```

### From a local file

```bash
python -m mcp_adapter generate \
  --spec examples/petstore.yaml \
  --output output/petstore-mcp
```

### Run the generated server

```bash
cd output/math-api-mcp
pip install -r requirements.txt
cp .env.example .env   # fill in your API key
python server.py       # starts on http://127.0.0.1:8000/mcp
```

---

## CLI Reference

### `generate`

| Option | Description |
|--------|-------------|
| `--spec PATH` | Path to local API spec file (OpenAPI YAML/JSON or Postman) |
| `--url URL` | URL to fetch OpenAPI/Swagger spec from |
| `--output, -o PATH` | Output directory for generated MCP server **(required)** |
| `--name TEXT` | Server name (defaults to API title) |
| `--use-k2` | Use AI reasoning to enhance schemas (K2 or Dedalus fallback) |
| `--block-destructive` | Block all DELETE/destructive tools |
| `--max-tools INT` | Max tools to generate (0 = unlimited) |
| `--allowlist TEXT` | Comma-separated tool names to include |
| `--denylist TEXT` | Comma-separated tool names to exclude |
| `-v, --verbose` | Enable debug-level logging |

### `inspect`

| Option | Description |
|--------|-------------|
| `--spec PATH` | Path to local API spec file |
| `--url URL` | URL to fetch OpenAPI/Swagger spec from |
| `--json-output` | Output as JSON instead of human-readable table |

---

## AI Reasoning (K2 Integration)

When `--use-k2` is enabled, the pipeline sends tool definitions to an AI for enhancement.

### Provider Priority (with automatic fallback)

1. **K2 (MBZUAI IFM)** â€” `K2_API_KEY` + optional `K2_BASE_URL`, `K2_MODEL`
2. **Dedalus API** â€” `DEDALUS_API_KEY` (uses `openai/gpt-4o-mini`)

If the first provider is unreachable, it automatically tries the next. If all fail, it falls back to the original tool definitions.

### What AI enhances

- **Tool names** â€” `addnumbers` â†’ `add_numbers`
- **Descriptions** â€” Generic â†’ clear, agent-friendly prose
- **Parameter docs** â€” Fills in missing descriptions
- **Safety classification** â€” Semantic re-evaluation beyond HTTP method

### Configuration (.env)

```bash
# Required: at least one reasoning provider
K2_API_KEY=IFM-your-key-here
K2_BASE_URL=https://your-k2-endpoint/v1   # optional
K2_MODEL=K2-Chat                           # optional

# Fallback provider
DEDALUS_API_KEY=dsk-your-key-here
```

---

## Environment Setup

Create a `.env` file in the project root:

```bash
DEDALUS_API_KEY=dsk-your-key-here    # For Dedalus MCP deployment + AI reasoning
K2_API_KEY=IFM-your-key-here         # For K2 reasoning (optional)
```

---

## Supported Input Formats

| Format | Status | Notes |
|--------|--------|-------|
| Swagger/OpenAPI URL | âœ… Supported | `--url http://host/openapi.json` |
| OpenAPI 3.x (YAML/JSON) | âœ… Supported | Best coverage â€” maps directly to tools |
| Swagger 2.x (YAML/JSON) | âœ… Supported | Auto-detected |
| Postman Collection v2.1 | âœ… Supported | Folders become tags |

---

## Safety & Permissions

Every generated tool is auto-classified:

- ğŸŸ¢ **read** â€” Safe, no side effects (GET, HEAD)
- ğŸŸ¡ **write** â€” Creates or modifies data (POST, PUT, PATCH)
- ğŸ”´ **destructive** â€” May permanently delete data (DELETE)

Additional safety features:
- **Allowlist/denylist** â€” Control exactly which tools are exposed
- **PII redaction** â€” Sensitive fields (password, token, ssn, etc.) are flagged
- **Description badges** â€” Tools are annotated with `[WRITES DATA]` or `[DESTRUCTIVE]`
- **`--block-destructive`** â€” One flag to remove all DELETE tools

---

## Generated Output

```
output/<name>/
â”œâ”€â”€ server.py          # Complete MCP server (dedalus_mcp)
â”œâ”€â”€ main.py            # Entry point for Dedalus deployment
â”œâ”€â”€ pyproject.toml     # Dependencies for deployment
â”œâ”€â”€ test_server.py     # Contract tests + schema validation
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ .env.example       # Environment variable template
```

---

## Project Structure

```
Dedalus/
â”œâ”€â”€ .env                     # API keys (DEDALUS_API_KEY, K2_API_KEY)
â”œâ”€â”€ requirements.txt         # Project dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ mcp_adapter/             # The adapter generator
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py          # python -m entry point
â”‚   â”œâ”€â”€ cli.py               # Click CLI (generate, inspect)
â”‚   â”œâ”€â”€ logger.py            # Structured coloured logging
â”‚   â”œâ”€â”€ models.py            # Pydantic data models
â”‚   â”œâ”€â”€ ingest.py            # OpenAPI/Postman/URL parsers
â”‚   â”œâ”€â”€ mine.py              # Capability mining
â”‚   â”œâ”€â”€ reasoning.py         # AI reasoning (K2 / Dedalus)
â”‚   â”œâ”€â”€ safety.py            # Safety classification + policy
â”‚   â””â”€â”€ codegen.py           # MCP server code generator
â”œâ”€â”€ test_application/        # Example target app
â”‚   â”œâ”€â”€ app.py               # Math REST API with Swagger UI
â”‚   â””â”€â”€ openapi.yaml         # OpenAPI 3.0 spec
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ petstore.yaml        # Petstore OpenAPI example
â””â”€â”€ output/                  # Generated MCP servers
```

---

## Guide: Creating an MCP Middleware from Scratch

This is a step-by-step guide to turn any REST API into an MCP server.

### Step 1: Build your target application

Create a standard REST API. Example (`test_application/app.py`):

```python
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route

async def add(request):
    data = await request.json()
    return JSONResponse({"result": data["a"] + data["b"]})

app = Starlette(routes=[Route("/add", add, methods=["POST"])])
```

### Step 2: Add an OpenAPI spec

Either write one manually (`openapi.yaml`) or serve it from your app:

```python
async def openapi_json(request):
    spec = yaml.safe_load(open("openapi.yaml"))
    return JSONResponse(spec)

# Add route: Route("/openapi.json", openapi_json)
```

### Step 3: Add Swagger UI (optional but recommended)

Serve Swagger UI at `/docs` so you can browse and test your API:

```python
async def swagger_ui(request):
    html = """<html><body>
    <div id="swagger-ui"></div>
    <script src="https://unpkg.com/swagger-ui-dist@5/swagger-ui-bundle.js"></script>
    <script>SwaggerUIBundle({url: '/openapi.json', dom_id: '#swagger-ui'})</script>
    </body></html>"""
    return HTMLResponse(html)
```

### Step 4: Generate the MCP server

```bash
# Without AI
python -m mcp_adapter generate \
  --url http://127.0.0.1:8001/openapi.json \
  --output output/my-api-mcp

# With AI-enhanced schemas
python -m mcp_adapter generate \
  --url http://127.0.0.1:8001/openapi.json \
  --output output/my-api-mcp \
  --use-k2
```

### Step 5: Test the generated server

```bash
cd output/my-api-mcp
cp .env.example .env
python server.py                    # starts MCP server on :8000
python test_server.py               # runs auto-generated tests
```

### Step 6: Test with a Dedalus AI agent

```python
from dedalus_labs import AsyncDedalus, DedalusRunner

client = AsyncDedalus()
runner = DedalusRunner(client)
result = await runner.run(
    input="What is 42 + 58?",
    model="openai/gpt-4o-mini",
    mcp_servers=["http://your-deployed-server/mcp"],
)
```

### Step 7: Deploy via Dedalus

1. Go to [dedaluslabs.ai/dashboard](https://www.dedaluslabs.ai/dashboard/servers)
2. Click **Add Server** â†’ connect your repo
3. Point to the `output/my-api-mcp/` directory (has `main.py` + `pyproject.toml`)
4. Set environment variables (`DEDALUS_API_KEY`, upstream API keys)
5. Deploy

### What happens under the hood

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Agent â”‚â”€â”€â”€â”€â–¶â”‚ MCP Server   â”‚â”€â”€â”€â”€â–¶â”‚ Your REST    â”‚â”€â”€â”€â”€â–¶â”‚ Database â”‚
â”‚ (Claude, â”‚     â”‚ (generated)  â”‚     â”‚ API          â”‚     â”‚ / Serviceâ”‚
â”‚  GPT, â€¦) â”‚â—€â”€â”€â”€â”€â”‚ dedalus_mcp  â”‚â—€â”€â”€â”€â”€â”‚              â”‚â—€â”€â”€â”€â”€â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    MCP               HTTP                 HTTP
    protocol          proxy                your logic
```

The generated MCP server acts as a **middleware**: it receives MCP tool calls from AI agents, translates them into HTTP requests to your API, and returns structured JSON responses.

---

## Roadmap

- [ ] SDK introspection (TypeScript / Python client libs â†’ tools)
- [ ] CLI help scraping (`--help` output â†’ tools)
- [ ] Docs URL scraping (HTML API docs â†’ tools)
- [ ] OAuth2 flow support in generated servers
- [ ] Multi-tenant deployment with per-user token vaults
- [ ] Upstream change detection (re-generate on new API versions)
- [ ] TypeScript server generation (in addition to Python)
